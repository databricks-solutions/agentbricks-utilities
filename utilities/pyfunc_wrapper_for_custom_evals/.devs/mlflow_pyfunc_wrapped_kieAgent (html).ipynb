{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c12b69f5-79d4-4fc2-a4b1-64310c797c4b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Wrap an `[KIE]agent` in a Custom Pyfunc + Explore Custom Evals\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "cab98476-3d62-4ad8-ac0c-0a563525198d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Workspace Setup:\n",
    "\n",
    "- Dependencies\n",
    "- UC Paths\n",
    "- Environment Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9fd8f496-1897-4e7d-99b2-999e8537e4c1",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "install dependencies"
    }
   },
   "outputs": [],
   "source": [
    "!pip install mlflow=3.1.4 databricks-agents=1.2.0 cloudpickle>=3.1.1\n",
    "\n",
    "dbutils.library.restartPython()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ad94fe7e-159e-47bd-88a8-3f83dc93c4a7",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "specify CATALOG-SCHEMA-MODEL names"
    }
   },
   "outputs": [],
   "source": [
    "## Widgets for CATALOG_NAME, SCHEMA_NAME, and MODEL_NAME\n",
    "# Please replace with your own values\n",
    "\n",
    "dbutils.widgets.text(\"CATALOG_NAME\", \"mmt\", \"Catalog Name\")\n",
    "dbutils.widgets.text(\"SCHEMA_NAME\", \"bricks\", \"Schema Name\")\n",
    "dbutils.widgets.text(\"MODEL_NAME\", \"tbct_wrapped_KIEagent\", \"Model Name\")\n",
    "\n",
    "# Retrieve the values from the widgets\n",
    "catalog_name = dbutils.widgets.get(\"CATALOG_NAME\")\n",
    "schema_name = dbutils.widgets.get(\"SCHEMA_NAME\")\n",
    "model_name = dbutils.widgets.get(\"MODEL_NAME\")\n",
    "\n",
    "\n",
    "## CREATE Catalog/Schema etc. if not already available...\n",
    "# Create catalog if it does not exist\n",
    "spark.sql(f\"CREATE CATALOG IF NOT EXISTS {catalog_name}\")\n",
    "\n",
    "# Create schema if it does not exist\n",
    "spark.sql(f\"CREATE SCHEMA IF NOT EXISTS {catalog_name}.{schema_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c8d70d58-5451-4ee4-a87c-03b07de6366e",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "env variables"
    }
   },
   "outputs": [],
   "source": [
    "import mlflow\n",
    "import mlflow.pyfunc\n",
    "import pandas as pd\n",
    "import requests\n",
    "import os\n",
    "\n",
    "## Set MLflow experiment\n",
    "# Get current user from Databricks context\n",
    "current_user = dbutils.notebook.entry_point.getDbutils().notebook().getContext().userName().get()\n",
    "\n",
    "# Set experiment path dynamically\n",
    "experiment_path = f\"/Workspace/Users/{current_user}/agentbricks_utilities_kie_test\"\n",
    "mlflow.set_experiment(experiment_path)\n",
    "\n",
    "## Set DATABRICKS_TOKEN\n",
    "token = dbutils.notebook.entry_point.getDbutils().notebook().getContext().apiToken().get()\n",
    "os.environ['DATABRICKS_TOKEN'] = token \n",
    "## for quick testing -- best practice to use Service Principal / PAT OR refactor to use with WorkspaceClient SDK -- serving.endpoint.query()\n",
    "\n",
    "## The assumption here is that you have already created an KIE endpoint with the desired response output.\n",
    "# We will wrap the {kie}Agent endpoint_url inside an MLflow Custom Pyfunc\n",
    "endpoint_url = f\"https://{workspace_url}/serving-endpoints/{endpoint_name}/invocations\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "870aa16b-d180-4e46-8fca-4b9f176b6af1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "---    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "07747ade-b454-409c-9087-325327fd46d8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Define a MLflow Custom Pyfunc `KIEwrapper` Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "52126618-18aa-4cbc-8381-873878b1768d",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Wrapper for API way"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import requests\n",
    "import pandas as pd\n",
    "import mlflow.pyfunc\n",
    "import json\n",
    "\n",
    "## type hints\n",
    "from typing import List, Dict\n",
    "\n",
    "class KIEwrapper(mlflow.pyfunc.PythonModel):\n",
    "    \"\"\"MLflow wrapper for KIE endpoints using chat message format.\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        endpoint_url: str,\n",
    "        token: str = None\n",
    "    ):\n",
    "        self.endpoint_url = endpoint_url\n",
    "        self.token = token or os.environ.get('DATABRICKS_TOKEN')\n",
    "        if not self.token:\n",
    "            raise ValueError(\"No token found in environment\")\n",
    "        self.headers = {\n",
    "            \"Authorization\": f\"Bearer {self.token}\",\n",
    "            \"Content-Type\": \"application/json\"\n",
    "        }\n",
    "\n",
    "    def load_context(self, context):\n",
    "        # Only use for context-dependent setup\n",
    "        pass\n",
    "\n",
    "    def predict(\n",
    "        self,\n",
    "        context,\n",
    "        model_input: pd.DataFrame \n",
    "        ) -> List[str]:\n",
    "        try:\n",
    "            if isinstance(model_input, pd.DataFrame):\n",
    "                texts = model_input['workorder_notes'].tolist()\n",
    "            elif isinstance(model_input, list):\n",
    "                texts = [str(item) for item in model_input]\n",
    "            else:\n",
    "                texts = [str(model_input)]\n",
    "\n",
    "            results = []\n",
    "            for text in texts:\n",
    "                payload = {\n",
    "                    \"messages\": [\n",
    "                        {\n",
    "                            \"role\": \"user\",\n",
    "                            \"content\": text\n",
    "                        }\n",
    "                    ]\n",
    "                }\n",
    "                response = requests.post(\n",
    "                    self.endpoint_url,\n",
    "                    headers=self.headers,\n",
    "                    json=payload,\n",
    "                    timeout=360\n",
    "                )\n",
    "                if response.status_code == 200:\n",
    "                    result = response.json()\n",
    "                    if 'choices' in result and len(result['choices']) > 0:\n",
    "                        extracted_content = result['choices'][0]['message']['content']\n",
    "                        results.append(extracted_content)\n",
    "                    else:\n",
    "                        results.append(result)\n",
    "                else:\n",
    "                    error_msg = f\"HTTP {response.status_code}: {response.text}\"\n",
    "                    results.append({\"error\": error_msg})\n",
    "            return results\n",
    "        except Exception as e:\n",
    "            return [{\"error\": str(e)}]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "386e57e9-baa1-4ccb-8196-afff38f148f0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d53cc4db-3b38-4603-af7b-0a14caca512f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Create or Load sample data for logging `KIE`wrapper model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "300404fa-2360-4ab6-8ec7-55660a31122c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "This example is specific to an KIE agent created for the use-case where certain attributes of device and maintenance workorder logs require extraction for downstream modelling. Sample input examples are used for illustrative purposes here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6d8c52f8-af80-41ff-bd21-6b0357a46403",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "sample data for illustration"
    }
   },
   "outputs": [],
   "source": [
    "# Sample de-identified data for illustration \n",
    "\n",
    "import random\n",
    "import pandas as pd\n",
    "\n",
    "sample_df = pd.DataFrame(\n",
    "    [\n",
    "        {\"workorder_id\": str(random.randint(100000, 999999)),\n",
    "        \"workorder_notes\": \"\"\"reported_condition: Reported Condition not listed additional_diagnosis: Component Wear problem_description: 2025-02-18 12:11:26 DRD 02/18/2025 Machine is very loud while in use per management Roy said to remove and metal shavings were found in centrifuge. work_performed_desc: XXX 2025-02-18 12:13:44 Unable to remove centrifuge to observe where shavings are coming from due to stripped centrifuge screw. XXX 2025-02-19 12:14:43 Requested photos were uploaded XXX 2025-03-20 08:38:22 Just following up with this. Do we have an ETA of when a Tech will be scheduled to come and get this taken care of? XXX 2025-03-27 08:16:33 Noticed AC Hook was mising. Part ordered. XXX 2025-04-03 09:29:15 Should we use ScrewGrab to remove the stripped screw and be available to take a look at the centrifuge? OO 4/23/25 Removed stripped screw using Dremel tool. Center did not have a spare centrifuge or motor, so spares have been ordered and the center technician can complete the repair. EMB 4/23/25 XXX 2025-04-24 13:49:06 Replaced AC Hook, Centrifuge and Centrifuge Motor per R&R. Performed Multifunction CCA Auto-Test and Fluid Test with pasing results.\"\"\"},\n",
    "\n",
    "        {\"workorder_id\": str(random.randint(100000, 999999)),\n",
    "         \"workorder_notes\": \"\"\"reported_condition: Blood Spill additional_diagnosis: problem_description: 2025-01-12 12:11:06 Loop break occurred during donation. While completing routine fluid spill cleaning, blood was found underneath the leak detector. XXX 2025-03-28 15:51:10 Damaged centrifuge work_performed_desc: XXX 2025-01-12 12:15:47 Leak detector was removed and replaced on device. Multifunction test was ran and failed due to high presure leak test. - AW 1/12/25 XXX 2025-03-09 07:10:22 High presure leak troubleshooting steps were performed. High presure air system was presurized to 98 PSI. After 3 minutes, les than 3 PSI was lost. Multifunction auto-test was ran again and it still failed due to high presure leak test. XXX 2025-03-18 06:03:10 High presure leak test troubleshooting steps were performed as followed. High presure air system was presurized to 98 PSI. After 3 minutes les than 3 PSI was lost. Proceeded to the les than 3 PSI lost section. The draw and return pump tracks were actuated and neither observed a los of greater than 3 PSI. All valves on the soft casette housing were actuated and none of those observed a los greater than 3 PSI. The saline valve when actuated never reached a los greater than 3 PSI. The collection valve when actuated observed a los greater than 3 PSI. The pneumatic tubing for the valve was reseated and when energized the collection valve still observed a los greater than 3 PSI. Module will need to be replaced per high presure leak test troubleshooting steps. XXX 2025-03-28 15:39:12 Replaced collection valve following R&R; Multifunction test pased XXX 2025-03-28 16:08:21 Replaced Centrifuge following R&R XXX 2025-03-28 16:13:30 Performed another multifunction test after replacing centrifuge; Test pased XXX 2025-03-28 16:26:52 Fluid test pased\"\"\"},\n",
    "\n",
    "        {\"workorder_id\": str(random.randint(100000, 999999)),\n",
    "         \"workorder_notes\": \"\"\"reported_condition: Alarm Codes additional_diagnosis: problem_description: 2025-02-19 13:03:21 the machine alarmed three times but alarmed for the fourth time removed from service 3106 alarm XXX 2025-03-02 11:59:02 Machine losing presurization due to faulty return pump asembly. Return pump rotor caused the device to leak air presure upon closing tracks due to wear. work_performed_desc: XXX 2025-02-20 14:26:13 Navigated to the multifunction CCA section of the hardware tab. Presurized the high-presure air system to 98 psi. Waited 3 minutes. Observed how much presure is lost. The los is les than 3 psi. Performed Multifunction CCA autotest with failed results. High presure system leak test failed. Escalated to [CompanyName] per Service Manual. XXX 2025-03-02 11:59:02 Removed and replaced return pump asembly per service manual. Multifunction and return pump auto-tests performed with both tests pasing. Centrifuge presure sensors calibrated. XXX 2025-03-02 15:29:49 Fluid test complete with pasing results.\"\"\"},\n",
    "        \n",
    "        {\"workorder_id\": str(random.randint(100000, 999999)),\n",
    "         \"workorder_notes\": \"\"\"reported_condition: Installation Request additional_diagnosis: Not Listed problem_description: 2025-04-14 09:35:16 AC Pump out of box failure. KJ 4/14/25 XXX 2025-04-17 14:43:47 Installation completed with all pasing results. When running the fluid test, the centrifuge was louder than usual. The fluid test still pased. Torqued the centrifuge per R & R and inspected it for abnormalities. Nothing unusual noted. Another fluid test was performed and the centrifuge was still louder than expected. Fluid test pased. work_performed_desc: XXX 2025-04-17 14:47:30 Escalated for troubleshooting advice on a loud centrifuge. Should we order a new centrifuge? XXX 2025-04-17 14:58:34 When centrifuge was commanded to maximum speed on hardware tab, no abnormal noise occurred. The noise only occurs with a separation set loaded in the machine. Inspected the rollers for damage. All loop rollers are spinning freely. Loop holder has no abnormalities noted. XXX 2025-04-18 07:33:08 Hello, you can reference WO-00420320 for details on AC pump out of box failure. Will work on uploading those pictures you requested. Thank you XXX 2025-04-18 08:44:17 The centrifuge hinge is slightly loose but nothing out of the ordinary. Compared the loosenes to other machines and it has the same amount of give as other in-service machines do. Pictures uploaded XXX 2025-04-18 08:47:38 Using the hardware tab, commanded the centrifuge to spin at 2400 RPM with a separation set loaded in the filler housing. No abnormal or loud noise occurred. No abnormal vibration. Loud noise only occurs when fluid enters the centrifuge.\"\"\"},\n",
    "        \n",
    "        {\"workorder_id\": str(random.randint(100000, 999999)),\n",
    "        \"workorder_notes\": \"\"\"reported_condition: Preventive maintenance is due additional_diagnosis: problem_description: Preventive Maintenance is due Damaged centrifuge, return pump rotor, and door liner. XXX 2/25/25 work_performed_desc: PM Completed per manufacturers instructions. XXX 2/25/25 Replaced centrifuge & motor. Replaced return pump rotor. Replaced door liner. XXX 2/25/25 250g weight used: S/N: 141258 Cal due date: 4/30/25\"\"\"},\n",
    "     ] \n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ff394872-e135-4982-b878-410db4efd576",
     "showTitle": false,
     "tableResultSettingsMap": {
      "0": {
       "dataGridStateBlob": "{\"version\":1,\"tableState\":{\"columnPinning\":{\"left\":[\"#row_number#\"],\"right\":[]},\"columnSizing\":{},\"columnVisibility\":{}},\"settings\":{\"columns\":{}},\"syncTimestamp\":1755661935384}",
       "filterBlob": null,
       "queryPlanFiltersBlob": null,
       "tableResultIndex": 0
      }
     },
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(sample_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fa1bd57c-b51a-47d7-80cd-d81d6c324f7e",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "sample data as pandas DF"
    }
   },
   "outputs": [],
   "source": [
    "train_df = sample_df.sample(frac=0.6, random_state=42)\n",
    "test_df = sample_df.drop(train_df.index)\n",
    "train_df, test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "77b0f3a0-42aa-4376-9240-9ff2a8474a0f",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "test custom pyfunc locally before logging"
    }
   },
   "outputs": [],
   "source": [
    "# Instantiate your custom PythonModel\n",
    "local_model = KIEwrapper(endpoint_url,token) ## wrapped_KIEagent # KIEwrapper()\n",
    "\n",
    "# Test the predict method directly\n",
    "response_output = local_model.predict(context=None, model_input=test_df)\n",
    "\n",
    "response_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ed9f720f-5380-41e7-a98b-4d20d419a57b",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "check/reformat response_ouptut"
    }
   },
   "outputs": [],
   "source": [
    "[json.loads(r) for r in response_output]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6d81a01b-4281-4999-a4c8-5855bf3d1d69",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Infer MLflow model input/output Signatures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "96d583c5-03d7-40db-bd10-707c9c26c0ca",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "infer mlflow model input/output signatures"
    }
   },
   "outputs": [],
   "source": [
    "from mlflow.models.signature import ModelSignature, infer_signature\n",
    "\n",
    "# signature = infer_signature(inputs=test_df, outputs=response_output)\n",
    "signature = infer_signature(model_input=test_df, \n",
    "                            model_output=local_model.predict(context=None, model_input=test_df)\n",
    "                           )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "45393074-288d-431c-b8d8-9486b1996181",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "show in/putputs datatype signature"
    }
   },
   "outputs": [],
   "source": [
    "signature"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3c93b84e-c5f2-47cb-9e4e-956627283064",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b0538b73-ad94-4fdc-8c46-954663b5d97c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### MLflow log custom pyfunc wrappedKIEagent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9b784bcc-b224-4d5e-86fc-39816db50d88",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "use with API KIEwrapper"
    }
   },
   "outputs": [],
   "source": [
    "# endpoint_url = \"https://adb-830292400663869.9.azuredatabricks.net/serving-endpoints/kie-935bbd40-endpoint/invocations\"\n",
    "\n",
    "## instantiate KIEwrapper()\n",
    "wrapped_KIEagent = KIEwrapper(endpoint_url, token) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "00789a0d-ea4d-4e25-820c-29bf14eed9ef",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "mlflow log the wrappedKIEagent"
    }
   },
   "outputs": [],
   "source": [
    "with mlflow.start_run() as run:\n",
    "    \n",
    "    model_info = mlflow.pyfunc.log_model(        \n",
    "        name=\"bricks_kie_agent\",  \n",
    "        python_model=wrapped_KIEagent,\n",
    "        input_example=test_df,\n",
    "        pip_requirements=[\"mlflow==3.1.4\", \"requests>=2.25.0\", \"pandas>=1.3.0\", \"cloudpickle>=3.1.1\"],\n",
    "        signature=signature,\n",
    "        # registered_model_name=\"{catalog}.{schema}.{model_name}\" ## can directly register it if confident -- it's advisable to test before doing it (for shipping the model registration code after testing you can register to UC directly)\n",
    "    )\n",
    "    \n",
    "    print(\"\\nModel logged successfully!\")\n",
    "    \n",
    "    # Load and test\n",
    "    model = mlflow.pyfunc.load_model(model_info.model_uri)\n",
    "    response = model.predict(test_df)\n",
    "    \n",
    "    print(f\"\\nmodel_response: {response}\")\n",
    "    print(f\"\\nmodel uri: {model_info.model_uri}\")\n",
    "    print(f\"\\nrun_id: {model_info.run_id}\")\n",
    "\n",
    "print(f\"\\nmlflow experiment_id: {run.info.experiment_id} -- model logging completed successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f841df57-2392-429f-b05a-bc8163d06203",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Model Response Output \n",
    "-- example parsing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ea504b42-4dd0-446a-baed-5c7a32056bfd",
     "showTitle": true,
     "tableResultSettingsMap": {
      "0": {
       "dataGridStateBlob": "{\"version\":1,\"tableState\":{\"columnPinning\":{\"left\":[\"#row_number#\"],\"right\":[]},\"columnSizing\":{},\"columnVisibility\":{}},\"settings\":{\"columns\":{}},\"syncTimestamp\":1755668837902}",
       "filterBlob": null,
       "queryPlanFiltersBlob": null,
       "tableResultIndex": 0
      }
     },
     "title": "example of parsing response output"
    }
   },
   "outputs": [],
   "source": [
    "import json\n",
    "from pyspark.sql import functions as f, types as t\n",
    "\n",
    "## convert response to sparkDF for parsing\n",
    "response_sdf = spark.createDataFrame(pd.DataFrame(response, columns = ['response']))\n",
    "# display(response_sdf)\n",
    "\n",
    "\n",
    "#'response_sdf' has a column 'response' with JSON strings -- extract schema from first row\n",
    "json_rdd = response_sdf.limit(1).select(\"response\").rdd.map(lambda row: row.response)\n",
    "inferred_df = spark.read.json(json_rdd)\n",
    "# display(inferred_df)\n",
    "\n",
    "## apply schema in from_json extraction to all rows\n",
    "parsed_df = response_sdf.withColumn(\"parsed\",\n",
    "                                    f.from_json(f.col(\"response\"), \n",
    "                                                inferred_df.schema\n",
    "                                                )\n",
    "                                    )\n",
    "parsed_df = parsed_df.select(\"*\", \"parsed.*\")                                    \n",
    "display(parsed_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c9ef168a-6dfa-434e-96d8-6c85e357a367",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4f44441d-f212-4b44-a8ef-d0ab7f96de06",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Register logged model to UC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6c1398d2-c54c-4cf1-88ac-3f7ebde36556",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "register model to UC"
    }
   },
   "outputs": [],
   "source": [
    "# Register the model in Unity Catalog : \"{catalog}.{schema}.{model_name}\"\n",
    "uc_model_name = f\"{catalog_name}.{schema_name}.{model_name}\"\n",
    "\n",
    "registered_model = mlflow.register_model(\n",
    "    model_uri=model_info.model_uri,\n",
    "    name=uc_model_name\n",
    ")\n",
    "\n",
    "print(f\"Model registered in UC as: {uc_model_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "80f50a78-e7b2-405c-ab74-8c7b74f5530d",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "check version"
    }
   },
   "outputs": [],
   "source": [
    "# registered_model.version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "953e5b58-1e2f-4ba4-9f87-9b7358b334e3",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "retrieve registered uc-model latest version"
    }
   },
   "outputs": [],
   "source": [
    "def get_latest_uc_model_version(uc_model_name):\n",
    "    from mlflow.tracking import MlflowClient\n",
    "\n",
    "    client = MlflowClient()\n",
    "    model_versions = client.search_model_versions(f\"name='{uc_model_name}'\")\n",
    "    latest_uc_model_version = max([int(mv.version) for mv in model_versions])\n",
    "    print(f\"Latest version for {uc_model_name}: {latest_uc_model_version}\")\n",
    "\n",
    "    return latest_uc_model_version\n",
    "\n",
    "# latest_uc_model_version = get_latest_uc_model_version(uc_model_name)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8831b354-2b20-47b1-b9ef-c9b9eb9185cf",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Test inferencing with UC registered model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8369c29c-10b0-49bb-b2bc-f4896fba0dbc",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "test inference as uc_model as pyfunc"
    }
   },
   "outputs": [],
   "source": [
    "import mlflow\n",
    "\n",
    "uc_model_uri = f\"models:/{uc_model_name}/{get_latest_uc_model_version(uc_model_name)}\"\n",
    "pyfunc_model = mlflow.pyfunc.load_model(model_uri=uc_model_uri)\n",
    "\n",
    "# The model is logged with an input example\n",
    "input_data = pyfunc_model.input_example\n",
    "\n",
    "# Verify the model with the provided input data using the logged dependencies.\n",
    "# For more details, refer to:\n",
    "# REF https://mlflow.org/docs/latest/models.html#validate-models-before-deployment\n",
    "\n",
    "# Use the loaded model's predict method directly\n",
    "predictions = pyfunc_model.predict(input_data)\n",
    "\n",
    "predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0173d220-3796-41b6-a260-0886bfbcd70e",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "test inference with uc_model as pandasUDF"
    }
   },
   "outputs": [],
   "source": [
    "import mlflow\n",
    "# import pandas as pd\n",
    "\n",
    "uc_model_uri = f\"models:/{uc_model_name}/{get_latest_uc_model_version(uc_model_name)}\"\n",
    "loaded_model = mlflow.pyfunc.load_model(model_uri = uc_model_uri)\n",
    "uc_result = loaded_model.predict(test_df) # list of dicts\n",
    "\n",
    "# Convert the result to a Pandas DataFrame\n",
    "uc_result_df = pd.DataFrame({'prediction': uc_result})\n",
    "\n",
    "display(uc_result_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "cc421756-48c0-49ec-a8d4-a1976cda5e7e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### Use the sparkUDF function to generate some inferences from full dataset for evals "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6f19dac0-76e5-4d49-980d-776bf6a6507c",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "test inference with uc_model as sparkUDF"
    }
   },
   "outputs": [],
   "source": [
    "import mlflow\n",
    "import json\n",
    "from pyspark.sql import functions as f, types as t\n",
    "# from pyspark.sql import Row\n",
    "\n",
    "## use sample_df\n",
    "# If sample_df is a Pandas DataFrame, convert it to Spark DataFrame\n",
    "if isinstance(sample_df, pd.DataFrame):\n",
    "    df = spark.createDataFrame(sample_df)\n",
    "else:\n",
    "    df = sample_df\n",
    "\n",
    "uc_model_uri = f\"models:/{uc_model_name}/{get_latest_uc_model_version(uc_model_name)}\"\n",
    "\n",
    "# Load model as a Spark UDF. Override result_type if the model does not return double values.\n",
    "model_spark_udf = mlflow.pyfunc.spark_udf(\n",
    "    spark,\n",
    "    model_uri=uc_model_uri\n",
    ")\n",
    "\n",
    "# Predict on a Spark DataFrame.\n",
    "df = df.withColumn(\n",
    "    'predictions',\n",
    "    model_spark_udf() ## with model signature this is simplified \n",
    "    )\n",
    "# display(df)\n",
    "\n",
    "## ---------------------------------------------------------------------------------------------\n",
    "\n",
    "## with predictions column -- we can parse out the nested json as before -- extract schema from first row:\n",
    "json_rdd = df.limit(1).select(\"predictions\").rdd.map(lambda row: row.predictions)\n",
    "inferred_df = spark.read.json(json_rdd)\n",
    "# display(inferred_df)\n",
    "\n",
    "## apply schema in from_json extraction to all rows\n",
    "parsed_df = df.withColumn(\"parsed\",\n",
    "                          f.from_json(f.col(\"predictions\"), inferred_df.schema)\n",
    "                         )\n",
    "parsed_df = parsed_df.select(\"*\", \"parsed.*\")                                    \n",
    "display(parsed_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c0e76d25-8444-40e8-838f-5682cd2e580a",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "save for later evals testing"
    }
   },
   "outputs": [],
   "source": [
    "# parsed_df.write.mode(\"overwrite\").saveAsTable(f\"{catalog_name}.{schema_name}.<table_name e.g. sample_predictions_parsed>\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8d2c5cc4-7310-474b-a237-b95248be4140",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f3d816a8-83f7-408e-8076-bd3012826a20",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### DOWNSTREAM Custom Evals "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9935cbba-556d-4d43-96c9-ca7cd6e73fc8",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "read in sample  data N predictions for eval"
    }
   },
   "outputs": [],
   "source": [
    "# eval_df = spark.table(\"mmt.bricks.tbct_sample_predictions_parsed\")\n",
    "eval_df = spark.table(f\"{catalog_name}.{schema_name}.<table_name e.g. sample_predictions_parsed>\")\n",
    "display(eval_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "47b9d139-9f9d-4d35-9fa0-02d4ec1cd50b",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "test formatting eval_data"
    }
   },
   "outputs": [],
   "source": [
    "# eval_data = [\n",
    "#     {\n",
    "#         \"inputs\": {\n",
    "#             \"workorder_id\": row[\"workorder_id\"],\n",
    "#             \"workorder_notes\": row[\"workorder_notes\"]\n",
    "#         },\n",
    "#         \"outputs\": {\n",
    "#             \"predictions\": row[\"predictions\"]\n",
    "#         }\n",
    "#     }\n",
    "#     for row in eval_df.select(\n",
    "#         \"workorder_id\",\n",
    "#         \"workorder_notes\",\n",
    "#         \"predictions\"\n",
    "#     ).collect()\n",
    "# ]\n",
    "# display(eval_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "59c6a35b-bba7-4336-b9e7-310ceee82319",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Set up UC model load + eval_data prep"
    }
   },
   "outputs": [],
   "source": [
    "# --------------------------------------------------------------\n",
    "#  Imports & helper utilities\n",
    "# --------------------------------------------------------------\n",
    "from mlflow.genai.scorers import scorer, Guidelines\n",
    "from mlflow.genai.judges import meets_guidelines\n",
    "import mlflow\n",
    "import json\n",
    "from typing import Dict, List\n",
    "\n",
    "# --------------------------------------------------------------\n",
    "#   Helper Function to fetch the latest production version of a registered model\n",
    "# --------------------------------------------------------------\n",
    "\n",
    "## use existing get_latest_uc_model_version defined above\n",
    "def get_latest_uc_model_version(uc_model_name):\n",
    "    from mlflow.tracking import MlflowClient\n",
    "\n",
    "    client = MlflowClient()\n",
    "    model_versions = client.search_model_versions(f\"name='{uc_model_name}'\")\n",
    "    latest_uc_model_version = max([int(mv.version) for mv in model_versions])\n",
    "    print(f\"Latest version for {uc_model_name}: {latest_uc_model_version}\")\n",
    "\n",
    "    return latest_uc_model_version\n",
    "\n",
    "# latest_uc_model_version = get_latest_uc_model_version(uc_model_name)  \n",
    "\n",
    "\n",
    "\n",
    "# --------------------------------------------------------------\n",
    "#  Load the UC-registered Agent Brick KIE model\n",
    "# --------------------------------------------------------------\n",
    "UC_MODEL_NAME = \"mmt.bricks.tbct_wrapped_KIEagent\"\n",
    "UC_MODEL_URI = f\"models:/{UC_MODEL_NAME}/{get_latest_uc_model_version(UC_MODEL_NAME)}\"\n",
    "wrapped_KIEagent = mlflow.pyfunc.load_model(model_uri=UC_MODEL_URI)\n",
    "\n",
    "# --------------------------------------------------------------\n",
    "#  Predict wrapper – traced, returns JSON string predictions\n",
    "# --------------------------------------------------------------\n",
    "\n",
    "@mlflow.trace\n",
    "def extract_workorder_notes(\n",
    "    workorder_id: str,\n",
    "    workorder_notes: str\n",
    ") -> Dict:\n",
    "    payload = {\n",
    "        \"workorder_id\": workorder_id,\n",
    "        \"workorder_notes\": workorder_notes\n",
    "    }\n",
    "    response = wrapped_KIEagent.predict(payload)\n",
    "    print(response)\n",
    "\n",
    "    raw_predictions = response\n",
    "    if hasattr(raw_predictions, \"tolist\"):\n",
    "        raw_predictions = raw_predictions.tolist()\n",
    "\n",
    "    return {\"predictions\": json.dumps(raw_predictions)}\n",
    "\n",
    "# --------------------------------------------------------------\n",
    "#  Prepare evaluation data – **flattened** \n",
    "# --------------------------------------------------------------\n",
    "# Every element MUST have an `\"inputs\"` key | `\"outputs\"`could be optional (?) – \n",
    "# MLflow will populate it after calling `predict_fn`.\n",
    "\n",
    "eval_data = [\n",
    "    {\n",
    "        \"inputs\": {\n",
    "            \"workorder_id\": row[\"workorder_id\"],\n",
    "            \"workorder_notes\": row[\"workorder_notes\"]\n",
    "        },\n",
    "        \"outputs\": {\n",
    "            \"predictions\": row[\"predictions\"]\n",
    "        }\n",
    "    }\n",
    "    for row in eval_df.select(\n",
    "        \"workorder_id\",\n",
    "        \"workorder_notes\",\n",
    "        \"predictions\"\n",
    "    ).collect()\n",
    "]\n",
    "\n",
    "# eval_data ## defined earlier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b813b6bf-d41d-4240-b267-21a0af37a3f7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Define Judges & Scorers\n",
    "\n",
    "- https://docs.databricks.com/aws/en/mlflow3/genai/eval-monitor/custom-judge/ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "806f19eb-8c01-4cb7-bfd2-2a1eb5765f2a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "#### Test a custom eval with pre-built `meets_guidelines` Judge: e.g. `Hallucination Guardrail` "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "42fb3047-ecd2-4cdd-9062-d1820e2484c4",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "explore custom evals with KIE agent registered UC model"
    }
   },
   "outputs": [],
   "source": [
    "# --------------------------------------------------------------\n",
    "#  Scorers – single‑arg dict signature (do NOT call it!)\n",
    "# --------------------------------------------------------------\n",
    "\n",
    "@scorer\n",
    "def hallucination_guardrail(inputs, outputs): ## name in such a way that True/Pass vs False/Fail is clear\n",
    "    \"\"\"\n",
    "    `example` is created by mlflow.genai.evaluate and looks like:\n",
    "    {\n",
    "        \"inputs\":  {...},          # the row you fed to predict_fn\n",
    "        \"outputs\": {\"predictions\": \"...\"}   # whatever your predict_fn returned\n",
    "    }\n",
    "    \"\"\"\n",
    "\n",
    "    guidelines=[\n",
    "        # \"The extracted information must not contain hallucinated details.\",\n",
    "        # \"The response must be based solely on the provided workorder notes.\"\n",
    "        \"The extracted information can include inferred insights but must be factually consistent with the provided context.\",        \n",
    "    ]\n",
    "\n",
    "    \n",
    "    # Run the built‑in guideline judge\n",
    "    return meets_guidelines(\n",
    "        name=\"hallucination_guardrail\",\n",
    "        guidelines=guidelines,\n",
    "        context={\n",
    "            \"request\": inputs[\"workorder_notes\"],\n",
    "            \"response\": json.loads(outputs[\"predictions\"])\n",
    "        },\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6e06d0ce-dd59-435b-8ab3-46e33135870e",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "test/run eval?"
    }
   },
   "outputs": [],
   "source": [
    "# --------------------------------------------------------------\n",
    "#  Run the evaluation\n",
    "# --------------------------------------------------------------\n",
    "# NOTE: The *list* of scorers must contain the callable, NOT a call.\n",
    "results = mlflow.genai.evaluate(\n",
    "    data=eval_data,\n",
    "    predict_fn=extract_workorder_notes,\n",
    "    scorers=[hallucination_guardrail, # <-- no parentheses!\n",
    "             ## add other svorers\n",
    "            ]   \n",
    ")\n",
    "\n",
    "# Show the result in the notebook (Databricks UI also gives a nice table)\n",
    "display(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7aedb09c-1b8b-460b-9444-b290af02fb1c",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "save results to UC Vols"
    }
   },
   "outputs": [],
   "source": [
    "# results.save(\"/Volumes/mmt/bricks/tbct_predictive_maintenance/workorder_eval_results_20250821\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "86366bb7-f8b5-494c-b403-8ec5e2e64231",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "!ls /Volumes/mmt/bricks/tbct_predictive_maintenance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f77e3de3-cba8-4be4-b940-d97a14bad793",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c1be2dc7-9973-46e0-9612-b1025c70c565",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "### To Include ADDITIONAL/ OTHER Custom/Prebuilt Metrics etc.\n",
    "\n",
    "\n",
    "- https://docs.databricks.com/aws/en/mlflow3/genai/eval-monitor/concepts/scorers\n",
    "- https://docs.databricks.com/aws/en/mlflow3/genai/eval-monitor/custom-scorers \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b075f2bf-74ca-45bf-aad5-86931cfbcbb8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "%md\n",
    "\n",
    "#### Test a custom eval with pre-built `LLM judges` "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "44d2be3f-7fef-4800-a87d-9894d2ac4a3a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "The is_* judges in mlflow.genai.judges are specialized functions that score different quality dimensions of LLM outputs. Each judge evaluates a specific aspect of the model's response or context, such as relevance, safety, grounding, correctness, or sufficiency. Here is what each judge scores\n",
    "\n",
    "- **`is_safe`**: Scores whether the content contains harmful, offensive, or toxic material.\n",
    "- **`is_relevant / is_context_relevant`**: Scores whether the context or response is directly relevant to the user's request, without deviating into unrelated topics.\n",
    "- **`is_grounded`**: Scores whether the response is grounded in the information provided in the context (i.e., not hallucinated).\n",
    "- **`is_correct`**: Scores whether the response is factually correct compared to provided ground truth.\n",
    "- **`is_context_sufficient`**: Scores whether the context provides all necessary information to generate a response that includes the ground truth for the given request.   \n",
    "\n",
    "These judges are used to automate and standardize the evaluation of LLM outputs for different use cases and can be wrapped in custom scorers for integration with MLflow's evaluation harness\n",
    "\n",
    "- https://learn.microsoft.com/en-us/azure/databricks/mlflow3/genai/eval-monitor/concepts/judges/\n",
    "- https://learn.microsoft.com/en-us/azure/databricks/mlflow3/genai/eval-monitor/concepts/judges/pre-built-judges-scorers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "14571060-9a09-4541-b6f5-5a800a19dbbd",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "predefined-llm-judges + scorers"
    }
   },
   "outputs": [],
   "source": [
    "#REF https://docs.databricks.com/aws/en/mlflow3/genai/eval-monitor/custom-scorers#example-2-wrap-a-predefined-llm-judge\n",
    "\n",
    "from typing import Any\n",
    "from mlflow.genai.judges import is_context_relevant, is_grounded\n",
    "from mlflow.genai.scorers import scorer\n",
    "\n",
    "@scorer\n",
    "def is_response_relevant(\n",
    "    inputs: dict[str, Any],\n",
    "    outputs: dict\n",
    ") -> dict:\n",
    "    user_query = inputs.get(\"workorder_notes\")\n",
    "    agent_response = outputs.get(\"predictions\")  # Use 'predictions' instead of 'answer'\n",
    "\n",
    "    if not user_query or not agent_response:\n",
    "        raise Exception(\"Missing input fields: response or workorder_notes.\")\n",
    "\n",
    "    return is_context_relevant(\n",
    "        request=user_query,\n",
    "        context={\"response\": agent_response},\n",
    "    )\n",
    "\n",
    "@scorer\n",
    "def is_grounded_scorer(\n",
    "    inputs: dict[str, Any],\n",
    "    outputs: dict\n",
    ") -> dict:\n",
    "    user_query = inputs.get(\"workorder_notes\")\n",
    "    agent_response = outputs.get(\"predictions\")  # Use 'predictions' instead of 'answer'\n",
    "\n",
    "    if not user_query or not agent_response:\n",
    "        raise Exception(\"Missing input fields: response or workorder_notes.\")\n",
    "\n",
    "    return is_grounded(\n",
    "        request=user_query,\n",
    "        response=agent_response,\n",
    "        context={\"response\": agent_response}\n",
    "    )\n",
    "\n",
    "\n",
    "\n",
    "predefinedllmjudge_scorer_eval_results = mlflow.genai.evaluate(\n",
    "                                                                data=eval_data,\n",
    "                                                                scorers=[is_response_relevant, \n",
    "                                                                        is_grounded_scorer]\n",
    "                                                              )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "11ab1aba-7777-41f9-a6cb-b5a189a49389",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "#### Test a custom eval with custom-prompt-based-judges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "587265db-5b53-43f9-bb2b-a714b87565f2",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "device_replacement_judge + scorer"
    }
   },
   "outputs": [],
   "source": [
    "#REF https://docs.databricks.com/aws/en/mlflow3/genai/prompt-version-mgmt/prompt-registry/evaluate-prompts#custom-prompt-based-judge\n",
    "\n",
    "from mlflow.genai.judges import custom_prompt_judge\n",
    "from mlflow.genai.scorers import scorer\n",
    "\n",
    "# Create a custom prompt judge for device replacement\n",
    "device_replacement_judge = custom_prompt_judge(\n",
    "    name=\"device_repair_and_replacement_compliance_with_reasoning\",\n",
    "    prompt_template=\"\"\"Evaluate if the following work order indicates a device repair and replacement of a new part is ambiguous.\n",
    "\n",
    "Workorder Notes: {{workorder_notes}}\n",
    "Predicted Action: {{part_replace_per_protocol}}\n",
    "\n",
    "Choose the appropriate rating:\n",
    "[[repaired and replaced]]: Device replacement of a new part took place\n",
    "[[not_replaced]]: Device replacement of a new part did NOT take place\n",
    "\n",
    "Explain your reasoning for the rating:\"\"\",\n",
    "    numeric_values={\n",
    "        \"repaired and replaced\": 1.0,\n",
    "        \"not_replaced\": 0.0\n",
    "    }\n",
    ")\n",
    "\n",
    "# Wrap the judge in a scorer\n",
    "@scorer\n",
    "def device_replacement_scorer(inputs, outputs, trace) -> bool:\n",
    "    \"\"\"Custom scorer that evaluates device replacement compliance.\"\"\"\n",
    "    workorder_notes = inputs.get(\"workorder_notes\", \"\")\n",
    "    part_replace = outputs.get(\"part_replace_per_protocol\", \"\")\n",
    "    result = device_replacement_judge(\n",
    "        workorder_notes=workorder_notes,\n",
    "        part_replace_per_protocol=part_replace\n",
    "    )\n",
    "    return result.value == 1.0  # True if repaired and replaced, False otherwise\n",
    "\n",
    "\n",
    "custompromptjudge_scorer_eval_results = mlflow.genai.evaluate(\n",
    "                                                                data=eval_data,\n",
    "                                                                scorers=[device_replacement_scorer]\n",
    "                                                              )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b6dbf9e6-0643-4d0f-8429-c5f78256e8c7",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "device_replacement_ambiguity_judge + scorer"
    }
   },
   "outputs": [],
   "source": [
    "#REF https://docs.databricks.com/aws/en/mlflow3/genai/prompt-version-mgmt/prompt-registry/evaluate-prompts#custom-prompt-based-judge\n",
    "\n",
    "from mlflow.genai.judges import custom_prompt_judge\n",
    "from mlflow.genai.scorers import scorer\n",
    "\n",
    "device_replacement_ambiguity_judge = custom_prompt_judge(\n",
    "    name=\"device_replacement_ambiguity\",\n",
    "    prompt_template=\"\"\"Evaluate if the following work order is ambiguous regarding device repair and replacement of a new device part:\n",
    "\n",
    "Workorder Notes: {{workorder_notes}}\n",
    "Ambiguity Flag: {{ambiguity_flag}}\n",
    "\n",
    "Choose the appropriate rating:\n",
    "[[ambiguous]]: The work order is ambiguous about device repair and replacement.\n",
    "[[not_ambiguous]]: The work order is NOT ambiguous about device repair and replacement.\"\"\",\n",
    "    numeric_values={\n",
    "        \"ambiguous\": 1.0,\n",
    "        \"not_ambiguous\": 0.0\n",
    "    }\n",
    ")\n",
    "\n",
    "@scorer\n",
    "def device_replacement_ambiguity_scorer(inputs, outputs, trace) -> bool:\n",
    "    workorder_notes = inputs.get(\"workorder_notes\", \"\")\n",
    "    ambiguity_flag = outputs.get(\"ambiguity_flag\", \"\")\n",
    "    result = device_replacement_ambiguity_judge(\n",
    "        workorder_notes=workorder_notes,\n",
    "        ambiguity_flag=ambiguity_flag\n",
    "    )\n",
    "    return result.value == 1.0  # True if ambiguous, False otherwise\n",
    "\n",
    "custompromptjudge_scorer_eval_results = mlflow.genai.evaluate(\n",
    "    data=eval_data,\n",
    "    scorers=[device_replacement_ambiguity_scorer]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c6b99533-00d8-426e-8b6c-db74c690ee98",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5735155c-18b0-4cfc-a6f9-47ea37539b2b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Combined Evaluations + Results "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d2e17ba6-4e6f-4403-b62c-27b9a1729911",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "combined_results = mlflow.genai.evaluate(\n",
    "                                         data=eval_data,\n",
    "                                         predict_fn=extract_workorder_notes,\n",
    "                                         scorers=[hallucination_guardrail, \n",
    "                                                  is_grounded_scorer,\n",
    "                                                  is_response_relevant,\n",
    "                                                  device_replacement_scorer,\n",
    "                                                  device_replacement_ambiguity_scorer\n",
    "                                                 ]\n",
    "                                        )\n",
    "\n",
    "display(combined_results)                                        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2756fab2-542d-4dfc-ba87-ee6e02e0e7ed",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a93170e5-35a0-43ff-8aa0-195a560e6fbe",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "NOTES:\n",
    "\n",
    "Evaluation with LLM-based judges (such as those in `mlflow.genai.judges` or c`ustom prompt judges`) can be `slow` because each evaluation typically sends a separate API request to the LLM endpoint for every row and every scorer. This means:\n",
    "\n",
    "If you have 100 rows and 3 scorers, you may be making 300 LLM calls.\n",
    "Each LLM call can take several seconds, depending on model, load, and network latency.\n",
    "LLM endpoints may have rate limits, further slowing batch processing.\n",
    "If you use complex prompt templates or large contexts, the LLM inference time increases.\n",
    "To speed up evaluation:\n",
    "\n",
    "Reduce the number of rows or scorers for initial testing.\n",
    "Use smaller models or endpoints with lower latency.\n",
    "Batch requests if your framework and endpoint support it.\n",
    "Cache results for repeated evaluations.\n",
    "This is a known limitation of LLM-based evaluation workflows.\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": {
    "hardware": {
     "accelerator": null,
     "gpuPoolId": null,
     "memory": null
    }
   },
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "3"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "mlflow_pyfunc_wrapped_kieAgent (html)",
   "widgets": {
    "CATALOG_NAME": {
     "currentValue": "mmt",
     "nuid": "0c8e0827-695b-42a5-a260-e1e7322501e5",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "mmt",
      "label": "Catalog Name",
      "name": "CATALOG_NAME",
      "options": {
       "widgetDisplayType": "Text",
       "validationRegex": null
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "widgetType": "text",
      "defaultValue": "mmt",
      "label": "Catalog Name",
      "name": "CATALOG_NAME",
      "options": {
       "widgetType": "text",
       "autoCreated": null,
       "validationRegex": null
      }
     }
    },
    "MODEL_NAME": {
     "currentValue": "tbct_wrapped_KIEagent",
     "nuid": "2f4a267c-3459-4e54-9a78-d0112e8c37a0",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "tbct_wrapped_KIEagent",
      "label": "Model Name",
      "name": "MODEL_NAME",
      "options": {
       "widgetDisplayType": "Text",
       "validationRegex": null
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "widgetType": "text",
      "defaultValue": "tbct_wrapped_KIEagent",
      "label": "Model Name",
      "name": "MODEL_NAME",
      "options": {
       "widgetType": "text",
       "autoCreated": null,
       "validationRegex": null
      }
     }
    },
    "SCHEMA_NAME": {
     "currentValue": "bricks",
     "nuid": "1e4beb0b-88bb-42ef-9416-4b0ebd8e0769",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "bricks",
      "label": "Schema Name",
      "name": "SCHEMA_NAME",
      "options": {
       "widgetDisplayType": "Text",
       "validationRegex": null
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "widgetType": "text",
      "defaultValue": "bricks",
      "label": "Schema Name",
      "name": "SCHEMA_NAME",
      "options": {
       "widgetType": "text",
       "autoCreated": null,
       "validationRegex": null
      }
     }
    }
   }
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
